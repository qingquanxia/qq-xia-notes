[
  {
    "objectID": "physics/discrete-time.html",
    "href": "physics/discrete-time.html",
    "title": "Part 2, Discrete Time",
    "section": "",
    "text": "This article is part of a series:\nThe purpose of this post is mainly to provide intuition about how signals are discretized. Please don’t lose sleep over how messy the math here is. The next article will provide a powerful tool which will immensely simplify many of the formulas in this article.\nThe modern truth is that even though the things we care about are analog, computers live in a digital world. Any signal processing done inside a computer must both be discrete in the time domain, and also represent values with limited precision.\nTo explain the math though, let’s assume for the remainder of the article that we can represent real numbers with infinite precision, and that we are only forced to discretize the time domain.\nWith the Fourier series as our predecessor, we now work out the discrete fourier transform (DFT).",
    "crumbs": [
      "Home",
      "Physics",
      "Part 2, Discrete Time"
    ]
  },
  {
    "objectID": "physics/discrete-time.html#the-dft",
    "href": "physics/discrete-time.html#the-dft",
    "title": "Part 2, Discrete Time",
    "section": "The DFT",
    "text": "The DFT\nConsider the set of functions \\(f: \\{0, \\cdots, N-1\\} {\\rightarrow}{\\bf C}\\). These form a complex inner product space, with their inner product defined as \\(\\braket{f|g} = \\sum_{n=0}^{N-1} f^*(n) g(n)\\). In fact this is nothing more than the vector space \\({\\bf C}^N\\).\n\\(\\braket{e^{2\\pi i k_1 n} | e^{2\\pi i k_2 n}} = \\sum_{n=0}^{N-1} e^{2\\pi i (k_2-k_1)n} = \\frac{1-e^{2\\pi i (k_2-k_1)N}}{1-e^{2\\pi i (k_2-k_1)}}\\). This is zero if and only if \\(2\\pi (k_2 - k_1)N = 2\\pi k\\) for some integer \\(k\\), that is, \\(k_2-k_1 = k/N\\). The intuition behind these formulas is that when the condition is satisfied, the terms in the sum form \\(N\\) evenly spaced rays shining from the origin of the complex plane, which add to zero because of their symmetry.\nHence, \\(\\{e^{2\\pi i (k/N)n}\\}_{k\\in {\\bf Z}}\\) is an orthogonal set of functions. However, any two functions where \\(k\\) differs by a multiple of \\(N\\) are actually the same function on our discrete set of points. This is because \\(e^{2\\pi i (0/N)n} = e^{2\\pi i (N/N)n} = 1\\) for all integers \\(n\\). This effect is known as aliasing.\n\n\n\n\n\n\nNoteAliasing\n\n\n\n\n\nAliasing is the same reason why in movies a racecar’s wheel will be seen slowly turning backwards even as the car is blazing at over 120mph down a freeway. A typical video sampling rate is 60 frames per second. This means that if the tire made 60 revolutions per second = 3600rpm, then on each frame of the video the wheel will be in exactly the same position. So 61 revolutions per second would alias down to 1 revolution per second, that is, the tire slowly crawling forward, whereas 59 revolutions per second would alias down to -1 revolution per second, that is, the tire slowly crawling backward.\n\n\n\nFigure from Arcak’s Berkeley EE16B Sp17 lecture notes\n\n\n\n\n\nBecause of aliasing, our actual orthonormal basis is \\(\\{\\frac{1}{\\sqrt{N}} e^{2\\pi i (k/N)n}\\}_{k \\in \\{0, \\cdots, N-1\\}}\\). It makes sense that we have \\(N\\) basis functions, since our original vector space was of dimension \\(N\\).\nWe now define our discrete fourier transform, which is standard in practically all software implementations:\n\\[\n\\boxed{\n    \\begin{aligned}\n    \\mathrm{DFT}[x(n)] &=\\sum_{n=0}^{N-1} e^{-2\\pi ikn/N} x(n) \\\\\n    \\mathrm{IDFT}[X(k)] &=\\frac{1}{N}\\sum_{n=0}^{N-1} e^{2\\pi ikn/N} X(k)\n    \\end{aligned}\n}\n\\]\nNote that instead of having a \\(1/\\sqrt{N}\\) in both transforms to make them symmetric, we’ve placed the entire \\(N\\) term in the inverse transform. This makes the transform non-unitary. The reasons for this, however, will become clear in the next article.\nSo far we’ve been using integer values for \\(n\\). If we place our domain of \\(N\\) points in the time-domain though, and change variables so \\(t = \\Delta t n\\) for some scale factor \\(\\Delta t\\), then our orthonormal functions become \\(\\{\\frac{1}{\\sqrt{N}} e^{2\\pi i (\\frac{k}{N\\Delta t})t}\\}_{k \\in \\{0, \\cdots, N-1\\}}\\), so that in the frequency domain our points are spaced \\(\\frac{1}{N\\Delta t}\\) apart.",
    "crumbs": [
      "Home",
      "Physics",
      "Part 2, Discrete Time"
    ]
  },
  {
    "objectID": "physics/discrete-time.html#the-dtft",
    "href": "physics/discrete-time.html#the-dtft",
    "title": "Part 2, Discrete Time",
    "section": "The DTFT",
    "text": "The DTFT\nSo far, our DFT can handle functions which periodic with period \\(N\\). What if we want to handle an aperiodic signal though? Suppose we have an audio CD recorder that is responsible for sampling an analog sound wave at 44.1kHz. Every 1/44.1kHz \\(\\approx\\) 22.7us, this ADC (analog-to-digital converter) will capture the amplitude at its input, round it to, say, the nearest 8-bit value, and then output it as a practically endless stream of numbers. In theory, it has been running since the beginning of time and will continue to run after the stars burn out.\nHow do we model this? Perhaps we could imitate what we did in the continuous case. Take our DFT scenario, and imagine the limit as \\(N{\\rightarrow}\\infty\\). But if we only extend the right side of our domain to infinity, we’re stuck with a left end which is fixed at 0. Should we perhaps consider \\(\\{-N, \\cdots N\\}\\) to be our domain instead? At this point I got bogged down in the details of the math and never completed it.\nFortunately, we don’t have to do this, because we had already derived the answer in the previous article! Take a look at the following figure, which summarizes all the things that we have derived so far. There is a pattern which allows us to immediately write down the equation.\n\nBecause of the wonderful duality of these transforms, we can pretty much copy the Fourier series transforms verbatim to arrive at the Discrete Time Fourier Transform (DTFT):\n\\[\n\\begin{align}\n\\mathrm{DFTF}[x(n)] &= \\sum_{n=-\\infty}^{\\infty} e^{-2\\pi i kn} x(n) \\\\\n\\mathrm{IDFTF}[X(k)] &= \\int_{k=0}^1 e^{2\\pi i kn} X(k) dk\n\\end{align}\n\\]\n\nTODO: z-domain\nMany people choose to think in terms of \\(z^n\\), where \\(z=e^{2\\pi ik}\\)\n\n\nThe Nyquist-Shannon theorem\nWe can now explain the outrageous claim of the Nyquist-Shannon sampling theorem. What it says is if you have a continuous function which in the frequency domain has bandwidth \\(F\\), as long as you sample it at a frequency greater than \\(F\\), you have preserved ALL the information of that function.\nFrom our sampled points, we can perfectly reconstruct the original signal with a magical process called sinc interpolation: essentially we multiply each sample by a time-shifted sinc function and add them all up. Intuitively this makes sense, since each sinc function is 1 at the sample and 0 for all other samples, so it acts like a sort of smooth “basis”, and changing the value of one sample will not affect the interpolation at the other samples.\n\n\n\nFigure from Arcak’s Berkeley EE16B Sp17 lecture notes\n\n\nWith the DTFT in hand though, we can now analyze this more rigorously. Let’s say our signal is \\(x(t)\\), and its Fourier transform is \\(X(f) = \\int_{-\\infty}^{\\infty} e^{-2\\pi ift}x(t)dt\\). Suppose \\(X(f)\\) lies completely within the interval \\([a,b]\\), where \\(a-b=F\\). In that case, we can represent \\(X(f)\\) by its Fourier series: \\[\\int_a^b e^{2\\pi itf}X(f)df = \\int_{-\\infty}^\\infty e^{2\\pi itf}X(f)df = x(t)\\] The assumption that \\(X(f)\\) is band-limited is crucial to the equality of the coefficients of the Fourier series and the Fourier transform. The Fourier series has as its domain \\(\\{\\frac{n}{b-a}\\}_{n\\in{\\bf Z}}\\). Hence the coefficients of the Fourier series is nothing more than \\(x(t)\\) sampled at a rate of \\(F\\). At the same time, we know that we can use these coefficients to perfectly reconstruct the frequency spectrum lying in the interval \\([a,b]\\), and hence perfectly reconstruct the original signal \\(x(t)\\).\n\nFrom a frequency domain perspective, what we are essentially doing to \\(X(f)\\) was multiplying it by a square window. But of course, in the time domain that corresponds to convolution by sinc….",
    "crumbs": [
      "Home",
      "Physics",
      "Part 2, Discrete Time"
    ]
  },
  {
    "objectID": "physics/discrete-time.html#additional-topics",
    "href": "physics/discrete-time.html#additional-topics",
    "title": "Part 2, Discrete Time",
    "section": "Additional topics",
    "text": "Additional topics\nWe glossed over the fact that not only are computers discretized in time, they are also limited in the precision they can represent real amplitudes. When they round to the nearest representable level, they introduce quantization noise. This noise is often modeled as Gaussian (even though it isn’t), and it has an average power relative to full scale, which must be carefully considered in system design. I might expand this section once I learn more about this.\nOne other fascinating topic is how the DFT is actually implemented in software. A naive coding of the formula would yield a runtime of \\(O(N^2)\\). In the 1960s, an algorithm was discovered that ran in time \\(O(N\\log N)\\). It was dubbed the title of “Fast Fourier Transform” (FFT). This discovery completely revolutionized the field of signal processing, and for the first time made it feasible to process in the digital domain. There is a very elegant explanation of the FFT which casts the problem as a polynomial multiplication problem to exploit very subtle symmetries.",
    "crumbs": [
      "Home",
      "Physics",
      "Part 2, Discrete Time"
    ]
  },
  {
    "objectID": "physics/continuous-fourier.html",
    "href": "physics/continuous-fourier.html",
    "title": "Part 1, The continuous world: Fourier series to Fourier transforms",
    "section": "",
    "text": "This article is part of a series:\nThere is something special about waves. You can see them as the oscillating ocean surface, bobbing ships up and down as they travel to far off places. You can see them when you strike a long metal hand-rail, as the metal vibrates and causes the pole to be blurry in your vision, and then hear them as the metal transfers its vibration into the pressure of the air. You can see them quite literally, because light is just a ripple in the invisible electromagnetic field vibrating at \\(10^{15}\\) cycles per second.",
    "crumbs": [
      "Home",
      "Physics",
      "Part 1, The continuous world: Fourier series to Fourier transforms"
    ]
  },
  {
    "objectID": "physics/continuous-fourier.html#the-exponential-function",
    "href": "physics/continuous-fourier.html#the-exponential-function",
    "title": "Part 1, The continuous world: Fourier series to Fourier transforms",
    "section": "The exponential function",
    "text": "The exponential function\nThe exponential function is defined as the power series\n\\[e^z = 1+z+\\frac{z^2}{2!}+\\frac{z^3}{3!}+\\cdots\\]\nIf you plot \\(e^{i\\theta}\\) in the complex plane, as you increase \\(\\theta\\) you will see a particle starting from the x-axis, rotating counterclockwise in a unit circle at constant speed. From this we define cosine and sine as the real and imaginary parts:\n\\[\\cos(\\theta) := {\\operatorname{Re}}(e^{i\\theta}) = \\frac{1}{2}(e^{i\\theta}+e^{-i\\theta})\\]\n\\[\\sin(\\theta) := {\\operatorname{Im}}(e^{i\\theta}) = \\frac{1}{2i}(e^{i\\theta}-e^{-i\\theta})\\]\nEuler’s formula summarizes this decomposition nicely:\n\\[e^{i\\theta} = \\cos(\\theta) + i\\sin(\\theta)\\]\nDifferentiating the power series yields a very special property for the exponential function:\n\\[{\\frac{\\partial }{\\partial t}}(e^t) = e^t\\]\n(I am restricting myself to functions \\({\\bf R}{\\rightarrow}{\\bf C}\\). This also holds true for \\({\\bf C}{\\rightarrow}{\\bf C}\\), but that requires complex analysis to explain, and isn’t really relevant right now). This gives us a really neat alternative definition: “The exponential function is defined as the unique eigenfunction of the differentiation operator with eigenvalue 1, such that \\(e^0=1\\)”.\nBecause it turns out pretty much every formula in physics is a differential equation, the operator \\(({\\frac{\\partial }{\\partial t}})\\) is extremely important. That is why the exponential function has been dubbed “the most important function in mathematics” (Walter Rudin). In general the derivative does messy things to your function. But if you can express your function as the sum of exponentials, now you’ve got some serious simplification power.",
    "crumbs": [
      "Home",
      "Physics",
      "Part 1, The continuous world: Fourier series to Fourier transforms"
    ]
  },
  {
    "objectID": "physics/continuous-fourier.html#inner-product-spaces-and-dirac-notation",
    "href": "physics/continuous-fourier.html#inner-product-spaces-and-dirac-notation",
    "title": "Part 1, The continuous world: Fourier series to Fourier transforms",
    "section": "Inner product spaces and Dirac notation",
    "text": "Inner product spaces and Dirac notation\nThe idea of inner products originates from the 3D Euclidean vectors used in classical mechanics. In that context, a vector \\(v\\) is an arrow in 3D space; if you place the tail of the arrow at the origin, you can associate the vector with the coordinates at the head, \\((v_1, v_2, v_3)\\). The dot product of two vectors \\(u,v\\) is defined as \\(u\\cdot v = u_1v_1+u_2v_2+u_3v_3\\). It essentially measures how aligned they are. Two vectors that are perfectly aligned will give maximum dot product; perpendicular will give 0; anti-parallel will give most negative dot product. Notice in particular that \\(v\\cdot v = |v|^2\\); in fact this property can be used to define the length of a vector. They are also used for decomposing a vector into coordinates. If you have an orthonormal basis \\(\\{e_1, e_2, e_3\\}\\), you can project my vector \\(v\\) onto the lines spanned by each of them, so that projecting it onto \\(e_1\\), for example, will give the vector \\((v\\cdot e_1)e_1\\). (By orthonormal basis, I mean a basis in which the vectors are orthogonal to each other and each of unit length.) Because these three basis vectors span the entire 3-dimensional space, adding the three projection vectors will perfectly reconstruct the original vector.\nBut no one is content sticking to three dimensions. The same concepts can be applied to vectors of arbitrary dimension, so that for \\(u,v\\in {\\bf R}^n\\), \\(u\\cdot v = \\sum_i u_iv_i\\). The real fun begins when we try to extend this concept to complex vectors, that is, to \\({\\bf C}^n\\). Because length is absolutely essential to geometry, we would like to preserve the idea that the product of a vector with itself is its length squared, in particular, that it is real. Hence we will define the product of two vector \\(u,v\\in {\\bf C}^n\\) as \\(\\sum_i u_i^* v_i\\). Hence, the product of \\(v\\) with itself is \\(\\sum_i v_i^* v_i = \\sum_i |v_i|^2\\), which makes sense.\nHere we formalize the concept of an inner product, which is a generalization of all the ideas expressed above. An inner product over a complex (or real) vector space \\(V\\) is defined as a function \\(\\braket{\\cdot | \\cdot}\\) mapping \\(V\\times V{\\rightarrow}{\\bf C}\\) which has these three properties:\n\nLinearity: \\(\\braket{u|a v + b w} = a\\braket{u|v} + b\\braket{u|w}\\)\nConjugate symmetry: \\(\\braket{u|v} = \\braket{v|u}^*\\)\nPositive definite: \\(\\braket{v|v}\\) is greater than zero if \\(v\\neq 0\\), and \\(0\\) otherwise. In particular, its value is real.\n\nThree quick notes:\nFirst of all, this definition works equally well with a real vector space, since the complex conjugate of a real number is just itself.\nSecond, properties 1 and 2 combine to give \\(\\braket{au + bv| w} = a^* \\braket{u|w} + b^* \\braket{v|w}\\). It can be tricky to remember that if we take the complex scalar out of the first argument, it must be complex conjugated.\nThird, these definitions seem more general than our previous complex-coefficient based way of calculating inner products, but it is actually not. If we are given a finite dimensional complex inner product space \\(V\\), we are guaranteed by the Gram-Schmidt procedure to find an orthonormal basis \\(\\{e_1,\\cdots,e_n\\}\\), where \\(\\braket{e_i|e_j}\\) is 0 if \\(i\\neq j\\) and 1 if \\(i=j\\). Hence the inner product of any two vectors is, by linearity, \\(\\braket{u|v} = \\braket{\\sum_i u_i e_i | \\sum_j v_j e_j} = \\sum_{i,j} u_i^* v_j \\braket{e_i | e_j} = \\sum_i u_i^* v_i\\), which is precisely our old definition.\nSo our new definition is still the same familiar product. However, it does not reference any particular basis, and it gives us a new way to apply inner products to the strange world of uncountably-infinite dimensional vector spaces.",
    "crumbs": [
      "Home",
      "Physics",
      "Part 1, The continuous world: Fourier series to Fourier transforms"
    ]
  },
  {
    "objectID": "physics/continuous-fourier.html#fourier-series",
    "href": "physics/continuous-fourier.html#fourier-series",
    "title": "Part 1, The continuous world: Fourier series to Fourier transforms",
    "section": "Fourier series",
    "text": "Fourier series\nTo keep the math consistent, in the pure mathy parts I will always use \\(t\\) as the independent variable, \\(x,y\\) as functions of \\(t\\), and \\(f\\) to denote frequency. Keep in mind the same math can be applied whether your independent variable is space or time. In the physics parts however I reserve the right to go crazy with variable names.\nConsider the space of square-integrable functions \\([a,b]\\rightarrow {\\bf C}\\), written \\(L^2([a,b], {\\bf C})\\), which consists of all functions \\(f: [a,b]\\rightarrow {\\bf C}\\) such that \\(\\int_a^b |f|^2 dt &lt; \\infty\\). It is a complex vector space, and we can define an inner product on it: \\(\\braket{x(t)|y(t)} := \\int_a^b x^*(t)y(t)dt\\). Now let’s consider what the inner product of two exponentials looks like: \\(\\braket{e^{2\\pi if_1 t}| e^{2\\pi if_2 t}} = \\int_a^b e^{2\\pi i(f_2-f_1)t}dt = [\\frac{1}{2\\pi i(f_2-f_1)}e^{2\\pi i(f_2-f_1)t}]_a^b\\). In order for this to be zero, we want \\(e^{2\\pi i(f_2-f_1)b} = e^{2\\pi i(f_2-f_1)a}\\), that is, \\(2\\pi (f_2-f_1)(b-a) = 2\\pi n\\) for some integer \\(n\\). So a necessary and sufficient condition for two exponentials to be orthogonal is that \\(f_2-f_1 = n/(b-a)\\) for some integer \\(n\\).\nHence we have found that the set of functions \\(\\{ \\frac{1}{\\sqrt{b-a}} e^{2\\pi ifx} \\}_{f=n/(b-a)}\\) for \\(n\\in{\\bf Z}\\) is an orthonormal set. It turns out this set is complete as well, meaning that any function in the vector space \\(L^2([a,b], {\\bf C})\\) can be expressed as a linear combination of these. This set is called a Hilbert basis. (For a proof of why it is complete, see “Mathematics of Classical and Quantum Physics” by Byron and Fuller.)\n\nPhysicists generally leave the difficult job of proving completeness of a given set of functions to the mathematicians. All orthonormal sets of functions normally occurring in mathematical physics have been proved to be complete.\n— John David Jackson, Classical Electrodynamics, 2nd edition, page 66\n\nWe have now decomposed any function on the interval into the sum of complex exponentials! This lets us do interesting things….\n\nPhysics aside: Guitar strings\nConsider a string of length \\(L\\) stretched tight between two fixed points. Suppose its mass per unit length is \\(\\lambda\\), and it is subject to a tension of \\(T\\).\n\nFrom the diagram above, we see that the y-component of force on the tiny bit of string is \\(-T\\frac{dy}{dx} + T(\\frac{dy}{dx} + \\frac{d^2y}{dx^2}dx) = T\\frac{d^2y}{dx^2}dx\\). Hence, by Newton’s second law \\(F=ma\\), we have \\(T\\frac{d^2y}{dx^2}dx = (\\lambda dx) (\\frac{d^2y}{dt^2})\\), which gives us a “wave equation”: \\(\\frac{d^2y}{dx^2} = \\frac{1}{T/\\lambda} \\frac{d^2y}{dt^2}\\).\n(For comparison, here are some other wave equations:\n\nSound waves: \\(\\nabla^2 p = \\frac{1}{v_s^2}\\frac{\\partial^2 p}{\\partial t^2}\\) where \\(v_s=\\sqrt{B/\\rho_0}\\) is the speed of sound, \\(B\\) is the bulk modulus of the fluid, and \\(\\rho_0\\) is the average density. \\(\\nabla^2 = {\\frac{\\partial^2 }{\\partial^2 x}} + {\\frac{\\partial^2 }{\\partial^2 y}} + {\\frac{\\partial^2 }{\\partial^2 z}}\\) is the Laplacian operator.\nEM waves (light): \\(\\nabla^2 \\bf{E} = \\frac{1}{c^2}\\frac{\\partial^2 \\bf{E}}{\\partial t^2}\\), where \\(c=1/\\sqrt{\\epsilon_0\\mu_0}\\) is the speed of light.\nde-Broglie matter waves (like free electrons) in quantum theory: \\(\\nabla^2 \\psi = \\frac{2m}{\\hbar} (-i\\frac{\\partial \\psi}{\\partial t})\\). (This looks a bit different from the others; it expresses the fact that matter waves do not have a constant phase-velocity, which leads to dispersion of different frequency components. But keep in mind the \\(i\\) is essentially a derivative, so it has sorta the same form).\nGravitational waves: In the weak field limit, general relativity can be approximated by a set of four equations that exactly parallel Maxwell’s equations (see gravitoelectromagnetism). Hence gravitational waves obey the same equation that electromagnetic waves do, that is, \\(\\nabla^2 \\bf{E_g} = \\frac{1}{c^2}\\frac{\\partial^2 \\bf{E_g}}{\\partial t^2}\\), where \\(\\bf{E_g}\\) is the gravitoelectric field (conventional gravitational field).\n\nThese equations are very different from the heat and diffusion equations, in which the time derivative is only first order. Hence heat and diffusion do not exhibit wave-like behavior.)\nWith foresight, I will define \\(v=\\sqrt{T/\\lambda}\\).\nTo solve this equation, let’s start by only looking for solutions of the form \\(y(x,t) = f(x) g(t)\\). This seems a ridiculously strict assumption at first, but we will see soon that it actually yields very general solutions.\nPlugging it in, we get \\({\\frac{\\partial^2 f}{\\partial^2 x}} g = \\frac{1}{v^2}f{\\frac{\\partial^2 g}{\\partial^2 t}}\\) so \\(\\frac{1}{f}{\\frac{\\partial^2 f}{\\partial^2 x}} = \\frac{1}{v^2}\\frac{1}{g}{\\frac{\\partial^2 g}{\\partial^2 t}}\\). Now here is a subtle point. The right side of the equation depends only on \\(t\\), while the right side of the equation depends only on \\(x\\). This means that both sides are equal to some constant. If it were not so, then you could change the value of the left hand side by moving \\(t\\) and keeping \\(x\\) constant; because of the equality, the right side will also be changed; but this is a contradiction, because the right side depended only on \\(x\\).\nLet’s call the constant \\(-k^2\\), and let’s define \\(\\omega := vk\\). Now we have essentially decomposed our PDE into two ODEs, which are simple to solve.\n\\[\n\\begin{align}\n\\frac{1}{f}{\\frac{\\partial^2 f}{\\partial^2 x}} &= -k^2 \\\\\n\\frac{1}{g}{\\frac{\\partial^2 g}{\\partial^2 t}} &= -\\omega^2\n\\end{align}\n\\]\nWe end up with the general solutions \\(f(x) = Ae^{ikx} + Be^{-ikx}\\) and \\(g(t) = Ce^{i\\omega t} + De^{-i\\omega t}\\). The boundaries conditions specify that both ends of the string are fixed at zero, hence \\(A+B=0\\) and \\(Ae^{ikL}+Be^{-ikL}=0\\). Substituting the first equation into the second gives \\(A(e^{ikL}-e^{-ikL})=0 \\Rightarrow e^{ikL}=e^{-ikL}\\Rightarrow e^{2ikL}=1\\), which implies \\(k=n\\pi/L\\) for some integer \\(n\\). We get as our final solution: \\[y(x,t) = \\sin(kx)e^{i\\omega t} + \\text{c.c.}\\] (The c.c. stands for complex-conjugate, and it is just there to make the solution real. Some people will set in place a convention that the final solution is complex, and it’s implicit that the physical solution is obtained by taking the real part. This is totally fine for linear systems, and is commonly used in discussing phasors in electrical circuits, however it gets a bit confusing when non-linearity comes into play, because \\({\\operatorname{Re}}(ab) \\neq {\\operatorname{Re}}(a){\\operatorname{Re}}(b)\\).)\nThe solutions we have found are called the standing waves, because they oscillate in place. They are the first, second, etc harmonics, all multiples of the first (i.e. fundamental) harmonic. They seem like special cases, until we realize that, like the Fourier series, we can decompose any function into a sum of these standing waves.\nNormalizing the position equations gives us a family: \\(f(x) = \\sqrt{2} \\sin(k_n x)\\) where \\(k_n = n\\pi /L\\). From the spectral theorem, these equations form a complete orthonormal basis because they are the eigenfunctions of the Hermitian operator \\({\\frac{\\partial^2 }{\\partial^2 x}}\\) acting on the subspace of functions which are fixed to zero at \\(x=0,L\\).\nNow for some numbers. I’m pretty clueless when it comes to music, so I’ll get by with the best info I have. Looking at https://tension.stringjoy.com/, an accoustic guitar with phosphor bronze strings typically has a 25.5 inch 0.012 string (whatever 0.012 means). That string will give a pitch of E4 (329.63 Hz) when a tension of 25.7 pounds is applied. Assuming that E4 is the fundamental frequency of the note when played, we have \\(v=\\frac{\\omega_1}{k_1} = \\frac{2\\pi (329.63 \\mathrm{Hz})}{\\pi / (25.5 \\mathrm{in})} = 427 \\mathrm{m/s}\\). Incidentally this implies that the linear mass density is \\(\\lambda = T/v^2 = 0.627 \\mathrm{g/m}\\).\nNow let’s say we pluck the string at \\(x=0.3L\\), so it looks like a triangle at \\(t=0\\). We can decompose our initial shape \\(f(x)\\) into a sum of our harmonics. We can attach an appropriate \\(e^{i\\omega t}\\) to each of these harmonics to get our full solution \\(y(x,t)\\). Here are some plots:\n\n\n\n{{&lt; rawhtml &gt;}}\n\n\nYour browser does not support the video tag.\n\n{{&lt; /rawhtml &gt;}}\n{{&lt; details \"Periodic boundary conditions (click to expand) (TODO)\" &gt;}} The problem of a guitar string fixed at its ends actually didn’t involve Fourier series at all. Indeed, the Fourier series on that interval isn’t suited for the desired boundary conditions, because they are not zero at \\(x=0,L\\). Our fundamental harmonic \\(\\pi/L\\) had a wavenumber twice that of the fundamental wavenumber of the Fourier series. However, Fourier series are suitable when we desire periodic boundary conditions, that is, the two ends of string are joined together so there is no more boundary. I imagine this is hard to do with strings while maintaining tension, so we can use instead a metal ring. (This sort of boundary condition is also useful when you have an endless periodic lattice of atomic nuclei, all in perfect position inside a perfect crystal, with no boundary for millimeters on end, while their free electrons zip across the interior.) In this case, the boundary conditions require \\(A+B = Ae^{ikL} + Be^{-ikL}\\) and \\(ikA - ikB = ikAe^{ikL}-ikBe^{-ikL}\\). Adding these two equation gives \\(1 = e^{ikL}\\), which requires that \\(k = 2\\pi n/L\\) for an integer \\(n\\), which is exactly the condition of our Fourier series. But these are not enough. We need addition info on whether the wave is travelling forwards or backwards.\n\n{{&lt; /details &gt;}}\nThe Fourier series, which can be thought of as an orthonormal basis for the vector space \\(L^2({\\bf S}^1, {\\bf C})\\), are not the only set of complete orthonormal functions. Another beautiful example is the spherical harmonics, which form an orthonormal basis for \\(L^2({\\bf S}^2, {\\bf C})\\) (\\({\\bf S}^2\\) is the unit sphere). These show up in the orbitals of the hydrogen atom.\n\nReal (Laplace) spherical harmonics \\(Y_{lm}\\) for \\(l=0,\\dots, 4\\) (top to bottom) and \\(m = 0,\\dots, 4\\) (left to right). They are functions on the unit sphere: white regions are positive, blue regions are negative. image credit",
    "crumbs": [
      "Home",
      "Physics",
      "Part 1, The continuous world: Fourier series to Fourier transforms"
    ]
  },
  {
    "objectID": "physics/continuous-fourier.html#fourier-transforms",
    "href": "physics/continuous-fourier.html#fourier-transforms",
    "title": "Part 1, The continuous world: Fourier series to Fourier transforms",
    "section": "Fourier transforms",
    "text": "Fourier transforms\nFourier series can be used to model all sorts of interesting boundary problems, from the electromagnetic field in your microwave (metal walls impose zero electric field boundary conditions), to the variation of sound in an accoustic concert hall, to the behavior of an electron trapped in an infinite potential well.\nThey run into a limitation, however. Time is essentially infinite in both directions. Space is also infinite in all directions. How do we model light waves from distant stars, which appear to have taken millions years to reach us?\nWe need to consider the limit as \\([a,b]\\) stretches out into an infinite interval.\nLet \\(a\\rightarrow -\\infty\\) and \\(b\\rightarrow +\\infty\\). Our inner product becomes \\(\\braket{x(t),y(t)} \\approx \\int_{-\\infty}^{+\\infty}x^*(t)y(t)dt\\). We can still write an arbitrary function \\(x(t)\\) as \\[\n\\begin{align}\nx(t) &= \\sum_{n=-\\infty}^\\infty  \\braket{\\frac{1}{\\sqrt{b-a}}e^{2\\pi i\\frac{n}{b-a}t'} | x(t')} \\frac{1}{\\sqrt{b-a}}e^{2\\pi i\\frac{n}{b-a}t} \\\\\n&\\approx \\int_{f=-\\infty}^\\infty \\braket{e^{2\\pi ift'} | x(t')} e^{2\\pi ift} df \\\\\n&= \\int_{f=-\\infty}^\\infty \\left(\\int_{t'=-\\infty}^\\infty e^{-2\\pi ift'} x(t') dt'\\right) e^{2\\pi ift} df\n\\end{align}\n\\]\nThere we have it! We have derived our Fourier transform: \\[\n\\boxed{\n    \\begin{aligned}\n    &X(f) = \\int_{-\\infty}^\\infty e^{-2\\pi ift} x(t)dt \\\\\n    &x(t) = \\int_{-\\infty}^\\infty e^{2\\pi ift} X(f)df\n    \\end{aligned}\n}\n\\]\nThese equations are remarkably symmetric. In fact, it turns out that \\(\\mathrm{FT}[x(t)] = \\mathrm{IFT}[x(-t)]\\).\nThis has its uses. For example, one question that the Fourier transforms immediately raise is: what is the Fourier transform of \\(e^{2\\pi if_0t}\\)? If we try plugging it in, it yields a strange integral. For any \\(f\\neq f_0\\), that part of the integral should be zero, since a complex exponential is, over the whole real line, zero on average. However, when \\(f=f_0\\), the integral is infinity. This kinda makes sense since a pure tone should yield an infinitely sharp response in the frequency domain.\nBut we can be more precise. What is the Fourier transform of a dirac delta? \\(\\mathrm{FT}[\\delta (t-t_0)] = \\int_{t=-\\infty}^\\infty e^{-2\\pi ift} \\delta(t-t_0)dt = e^{-2\\pi ift_0}\\). But from duality, this means \\(\\mathrm{FT}[e^{2\\pi if_0t}] = \\delta(f-f_0)\\)!\n\nProperties:\nShamelessly copied from textbooks:\n\n\\(\\mathrm{FT}[ax_1(t) + bx_2(t)] = a\\mathrm{FT}[x_1(t)] + b\\mathrm{FT}[x_2(t)]\\) (linearity)\n\\(\\mathrm{FT}[x(at)] = \\frac{1}{|a|} X(\\frac{f}{a})\\)\n\\(\\mathrm{FT}[x(t)] = \\mathrm{IFT}[x(-t)]\\) and \\(\\mathrm{IFT}[X(f)] = \\mathrm{FT}[X(-f)]\\) (duality)\n\\(\\mathrm{FT}[x(t) * y(t)] = X(f)Y(f)\\) (convolution)\n\nCommon transforms:\n\n\\(\\mathrm{FT}[e^{2\\pi f_0 t}] = \\delta(f - f_0)\\)\n\\(\\mathrm{FT}[\\mathrm{sinc}(t)] = w_{-1/2,1/2}(f)\\) where \\(\\mathrm{sinc}(t) = \\frac{\\sin(\\pi t)}{\\pi t}\\) and the window function \\[ w_{a,b}(t) = \\begin{cases}\n1 & a\\leq t\\leq b \\cr\n0 & \\text{otherwise}\n\\end{cases}\n\\]\n\\(\\mathrm{FT}[u(t)] = \\frac{1}{2\\pi if} + \\frac{1}{2}\\delta(f)\\) (see Hilbert transform)\n\n\n\nA different perspective: quantum mechanics and momentum space\nI first learned about the Fourier transform in quantum mechanics, the way my professor presented it, I did not even realize it was a transform! Its description was so natural and intuitive. In this section I’ll quickly recite the basic rules in quantum mechanics for context, but the math can be applied to surprisingly many other physical systems.\nThere are five postulates in quantum mechanics:\n1 The state of a physical system is represented by a vector in a complex Hilbert space. (A Hilbert space is just an inner-product space whose norm makes it a complete metric space. All the familiar finite dimensional spaces are Hilbert spaces. The set of all functions \\(f: {\\bf R}{\\rightarrow}{\\bf C}\\) that are square-integrable in that \\(\\braket{f|f} = \\int_{-\\infty}^\\infty f^*(x)f(x) dx &lt; \\infty\\), i.e. \\(L^2({\\bf R}, {\\bf C})\\), is also a Hilbert space).\n2a A physical observable (e.g. energy, momentum, angular momentum, position, spin) is represented by a Hermitian linear operator on the Hilbert space. When you make a measurement of the observable using any sort of apparatus, it is only possible to observe an eigenvalue of the Hermitian operator.\n2b [not relevant to us] Measurement probability postulate.\n2c [not relevant to us] Collapse postulate.\n3 [not relevant to us] Time evolution.\nFor a particle that is free to travel along the entire 1-dimensional line, the Hilbert space we are interested in is \\(L^2({\\bf R}, {\\bf C})\\).\nThere are two operators we are interested in. The position observable is given by the operator \\(f(x) \\mapsto xf(x)\\), that is, multiplication by \\(x\\). It is easy to check that this is linear and Hermitian. Its eigenvalues consist of the entire real line. An eigenvalue \\(c\\) has eigenvector \\(\\delta(x-c)\\). Because its eigenvectors form an orthonormal basis for the Hilbert space, we can express any wave \\(\\psi(x)\\) in its basis. In this case it is trivial: \\(\\ket{\\psi} = \\int_x \\psi(x) \\ket{\\delta(x-c)} dx\\).\nThe momentum observable, on the other hand, is the operator \\(\\frac{\\hbar}{i} (d/dx)\\). Again, this is both linear and Hermitian. See the spectral theorem page for an explanation of why it is Hermitian. An eigenvector \\(\\ket{p}\\) with eigenvalue \\(p\\) must be a solution of the differential equation \\(\\frac{\\hbar}{i} (d/dx)\\phi(x) = p\\phi(x)\\). Its solution is \\(\\ket{p} = e^{ipx/\\hbar}\\). Hence the eigenvectors of the momentum observable consist of the set of exponential functions. Note that \\(\\hbar = \\frac{h}{2\\pi}\\) is the reduced plank constant, and \\(h\\approx 6.6\\times 10^{-34} \\mathrm{J}\\cdot\\mathrm{s}\\) is the regular plank constant. If we choose to use so called “natural units” such that \\(h=1\\), then the momentum \\(p\\) corresponds exactly to the frequencies that we use in our official definition of the Fourier transform!\nSo we have two different sets of orthonormal bases for our Hilbert space. Getting from one to the other is a simple matter of changing bases. The operator \\(\\int_p \\ket{p}\\bra{p} dp\\) is called a “resolution of the identity”, because it is in fact the identity; it merely breaks our vector up as a linear combination of the basis and then sums all the components back together again, giving us original vector. If we expand it out, what it is really saying is that \\[\n\\ket{\\psi} = \\left(\\int_p \\ket{p}\\bra{p} dp\\right)\\ket{\\psi} = \\int_p e^{ipx/\\hbar} \\braket{p|\\psi}dp = \\int_p e^{ipx/\\hbar} \\left(\\int_{x'} e^{-ipx'/\\hbar}\\psi dx'\\right)dp\n\\]\nBut this is just our FT/IFT! At its heart, the Fourier transform is nothing more than a change in basis.",
    "crumbs": [
      "Home",
      "Physics",
      "Part 1, The continuous world: Fourier series to Fourier transforms"
    ]
  },
  {
    "objectID": "physics/continuous-fourier.html#systems-and-convolution",
    "href": "physics/continuous-fourier.html#systems-and-convolution",
    "title": "Part 1, The continuous world: Fourier series to Fourier transforms",
    "section": "Systems and convolution",
    "text": "Systems and convolution\nA system is a black-box which takes in an input \\(x(t)\\) and spits out an output \\(y(t)\\). A linear system is a system that preserves linearity, that is, if it maps \\(x_1(t) \\mapsto y_1(t)\\) and \\(x_2(t)\\mapsto y_2(t)\\), then it maps \\(ax_1(t)+bx_2(t)\\mapsto ay_1(t)+by_2(t)\\). Its inputs live in some sort of vector space, say \\(L^2({\\bf R}, {\\bf C})\\). In that case, a linear system is a linear transformation from input space to output space, \\(L^2({\\bf R}, {\\bf C}) {\\rightarrow}L^2({\\bf R}, {\\bf C})\\). Linear transformations are completely described by their action on the basis, so we can capture all the information about the linear system in a function \\(h(t,t')\\) where \\(\\delta(t-t') \\mapsto h(t,t')\\).\nA linear time-invariant (LTI) system is one which is also time-invariant, that is, if \\(x(t) \\mapsto y(t)\\), then \\(x(t-t_0) \\mapsto y(t-t_0)\\). The neat thing about these kinds of systems is that since our dirac-delta basis was really just the set of all time-shifted versions of the dirac-delta, we can capture the entire behavior of the system by seeing where it maps \\(\\delta(t)\\). The response to the input \\(\\delta(t)\\) is usually denoted \\(h(t)\\). For any input \\(x(t)\\), we can find its response: \\[\\begin{align}\nAx(t) &= A\\int_{t'} x(t)\\delta(t-t')dt' \\\\\n&= \\int_{t'} x(t) A\\delta(t-t')dt' \\\\\n&= \\int_{t'} x(t) h(t-t')dt' \\\\\n&:= x(t) * h(t)\n\\end{align}\\]\n\nExample: electric fields and water molecules (TODO)\nFor concreteness, let’s use the example of an electromagnetic wave (microwave) hitting a glass of water. Water is a polar molecule: the oxygen atom is more electronegative than the hydrogen atoms, and pulls more of the negative charge to one end. This means each water molecule has a dipole moment. At room temperature, the molecules jiggle around randomly, their dipole moments executing an exotic dance. When an electric field is applied though, the dipole moments tend to align with the electric field. By applying an electric field \\(\\mathbf{E}\\), we have induced a polarization \\(\\mathbf{P}\\). In the case of a constant electric field, this relation is typically linear, so that \\(\\mathbf{P} = \\chi_E \\mathbf{E}\\). \\(\\chi_E\\) is the electric susceptibility of water.\nIt seems that we are done. Our solution is rather boring. For an input \\(\\mathbf{E}\\), our response \\(\\mathbf{P}\\) is just multiplying it by a constant. But this is not the whole picture. Imagine applying an electric pulse of infinitely short width, \\(\\mathbf{E_0} \\delta(t)\\). When the pulse hits water, the poor molecules don’t have time to instantaneously swivel in the direction of the field. After all, they still have to obey the laws of Newtonian mechanics: bodies at rest tend to stay at rest. They experience an instantaneous impulse, and afterwards, nothing. The response becomes \\(\\mathbf{P_0} h(t)\\).\nNow we transform into the frequency domain, \\(H(f)\\). This produces a spectrum.",
    "crumbs": [
      "Home",
      "Physics",
      "Part 1, The continuous world: Fourier series to Fourier transforms"
    ]
  },
  {
    "objectID": "math/fundamental-theorem-alg.html#visualizing-complex-polynomials",
    "href": "math/fundamental-theorem-alg.html#visualizing-complex-polynomials",
    "title": "3 Proofs of the Fundamental Theorem of Algebra",
    "section": "Visualizing complex polynomials",
    "text": "Visualizing complex polynomials\nReal polynomials are easy to visualize. They map the reals into the reals, so that the graph is a curve in \\({\\bf R}^2\\), which we can draw on a sheet of paper. A complex polynomial, however, maps the complex plane into the complex plane, so that its graph is 4-dimensional, hence impossible to plot.\nIn order to not overwhelm our low-dimensional brains, we can look at how the polynomial transforms circles. In the plots below, we’ve focused on a one-dimensional circle in the domain with radius \\(R=1\\), which lives in the complex plane. We color the circle so we can easily identify its shape, then transform it with the polynomial into its image, which also lives in the complex plane.\nThe simplest polynomials of the form \\(z^n\\) will transform the unit circle by winding it \\(n\\) times around the origin.\nA polynomial with real coefficients will appear symmetric with respect to reflection about the x-axis. This is because if \\(p\\) is a polynomial with real coefficients, then \\(p(z) = p(z^*)\\).\nIf \\(|z|\\) is small, then \\(p(z)\\) will be dominated by the lowest order term, and all other terms will be swamped out. This is due to the simple fact that if \\(z = 1/100\\), for example, then \\(z^1\\) is a hundred times larger than \\(z^2\\), which is in turn a hundred times larger than \\(z^3\\). How small \\(|z|\\) has to be before the lowest order term dominates depends on the values of the coefficients, but for any given polynomial, we are guaranteed some small number below which the lowest order term will dominate.\nIf \\(|z|\\) is large, then \\(p(z)\\) will be dominated the highest order term for similar reasons. If \\(z=100\\), then \\(z^3\\) is a hundred times larger than \\(z^2\\), which is in turn a hundred times larger than \\(z\\).",
    "crumbs": [
      "Home",
      "Math",
      "3 Proofs of the Fundamental Theorem of Algebra"
    ]
  },
  {
    "objectID": "math/fundamental-theorem-alg.html#the-theorem",
    "href": "math/fundamental-theorem-alg.html#the-theorem",
    "title": "3 Proofs of the Fundamental Theorem of Algebra",
    "section": "The Theorem",
    "text": "The Theorem\nEvery non-constant complex polynomial has a root.",
    "crumbs": [
      "Home",
      "Math",
      "3 Proofs of the Fundamental Theorem of Algebra"
    ]
  },
  {
    "objectID": "math/fundamental-theorem-alg.html#proof",
    "href": "math/fundamental-theorem-alg.html#proof",
    "title": "3 Proofs of the Fundamental Theorem of Algebra",
    "section": "Proof",
    "text": "Proof\n\nAsymptotics based proof (real analysis)\nLet \\(p(z) = a_0 + a_1 z^1 + \\cdots + a_N z^N\\) be a polynomial, where \\(N&gt;0\\). Consider the closed disc of radius \\(R\\) centered at the origin. As \\(R\\) grows indefinitely, the minimum value of \\(|p(z)|\\) on the boundary of the disc will grow indefinitely as well. Let’s pick \\(R\\) large enough so that it contains a point \\(z_0\\) for which \\(|p(z)|\\) is minimum (such a point must exist because compact sets map to compact sets, which is the generalization of the extreme value theorem, which says closed bounded intervals map to closed bounded intervals). Now shift \\(z_0\\) to the origin, so we have \\(q(z) = p(z+z_0) = b_0 + b_1 z^1 + \\cdots + b_N z^N\\). Suppose for contradiction that \\(b_0\\neq 0\\). Let’s take \\(b_0=1\\) without loss of generality.\nIf \\(b_1\\) were nonzero, then let \\(z=(-1)\\epsilon\\) for some small \\(\\epsilon&gt;0\\). Because the first-degree term dominates, we have found a \\(p(z)\\) which is less than our minimum at the origin, a contradiction. Hence we conclude \\(b_1=0\\).\nIf \\(b_2\\) were nonzero, then let \\(z=(-1)^(1/2) \\epsilon\\) for some small \\(\\epsilon&gt;0\\). Because the second-degree term dominates in the absence of \\(b_1\\), we have found a \\(p(z)\\) which is less than our minimum at the origin, a contradiction. Hence we conclude \\(b_2=0\\).\nIf \\(b_3\\) were nonzero, then let \\(z=(-1)^(1/3) \\epsilon\\) for some small \\(\\epsilon&gt;0\\). Because the third-degree term dominates in the absence of \\(b_1, b_2\\), we have found a \\(|p(z)|\\) which is less than our minimum at the origin, a contradiction. Hence we conclude \\(b_3=0\\).\n…\nIn this way we can conclude that all terms except for the constant term are 0, which contradicts our assumption that \\(p(z)\\) is non-constant. Hence \\(b_0=0\\), so that \\(q(0) = 0\\), so \\(p(z_0) = 0\\).\n\n\nRubber band proof (algebraic topology)\nLet \\(p(z)\\) be a non-constant polynomial. Let \\(A_R\\) be the circle of radius \\(R\\), and consider its image under \\(p\\). When \\(R=0\\), \\(A_R\\) consists only of the origin, and \\(p(A_R)\\) consists only of \\(p(0)\\). If \\(p(0)=0\\), we are done. Otherwise, for small \\(R\\), \\(p(A_R)\\) is a small, possibly tightly-wound rubber band that does not enclose the origin. When \\(R\\) expands though, there is a point when the \\(a_N z^N\\) term dominates and \\(p(A_R)\\) looks like almost a perfect circle enclosing the origin. There must have been a moment in the expansion when the rubber band touched the origin.\n\n\nAs a trivial corollary to an obscure theorem (complex analysis)\nLiouville’s theorem: A bounded complex-valued function which is holomorphic (aka complex-differentiable) on the entire complex plane must be constant.\nCorollary: Let \\(p(z)\\) be a non-constant polynomial. Suppose for contradiction it does not have any roots. Consider the closed disc of radius \\(R\\) centered at the origin. Grow \\(R\\) large enough so that outside the disc, \\(|p(z)|\\) is greater than \\(M\\). Because the closed disc is compact, \\(|p(z)|\\) has a minimum value \\(m\\) inside the disc, which is greater than 0 per our assumption. Then \\(1/p(z)\\) is holomorphic and bounded with magnitude at most \\(1/m\\), so that by Liouville’s theorem, it is constant, so \\(p(z)\\) is also constant, a contradiction.\n\n\nGalois theory based proof (todo)",
    "crumbs": [
      "Home",
      "Math",
      "3 Proofs of the Fundamental Theorem of Algebra"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "QQ Xia",
    "section": "",
    "text": "QingQuan Xia’s personal website"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! My name is QingQuan Xia.\nA little bit about me:\nI am a Christian who is saved through grace by the Lord Jesus Christ and holds that the Bible is the perfect literal word of God. I love discussing about the Bible with anyone who is interested!\nI graduated from UC Berkeley in 2022 as an Engineering Physics and EECS major, and then worked at SpaceX as a software engineer on the Direct-to-cell Starlink team for 3 years, writing code for the PHY/MAC layers of the 4G LTE protocol. However, because I really like doing physics, I’m applying for a PhD as of Nov 2025.\nThere are many mathematical tools I’ve learned and honed over the years that I want to gather in one place. As I continue to learn things, I’m planning to write them down as notes on this website so that I can refer back to them in the future and hopefully benefit a few others as well.\nWould be incredibly happy to receive feedback or questions! You can email me at “qingquanxia at gmail dot com”."
  },
  {
    "objectID": "physics/dirac-comb.html",
    "href": "physics/dirac-comb.html",
    "title": "Part 3, Unifying the continuous and discrete worlds: the dirac comb",
    "section": "",
    "text": "This article is part of a series:\nThe Dirac comb is defined as \\(d_c(t) = \\sum_{n=-\\infty}^{\\infty} \\delta(t-n)\\). It is an infinite train of impulses, each spaced exactly one unit apart. To make it spaced \\(\\Delta t\\) apart, we simply use \\(d_c(t/\\Delta t)\\). Notice how this not only adjusts the spacing, but also adjusts the intensity of the impulses, so that on average, integrating over a unit interval will always give you 1. This is a nice property which is analogous to firing paintball pellets at a cart. You can fire slowly with heavy powerful pellets, or you can fire rapidly with small wimpy pellets, but the rate at which momentum is imparted to the cart is the same. In fact, in the limit \\(\\Delta t\\rightarrow 0\\), we have \\(d_c(t/\\Delta t)\\rightarrow 1\\), analogous to hosing the cart down with a continuous stream of water instead of shooting it with discrete pellets.\nThe Fourier transform of \\(d_c(t)\\) turns out to be itself, that is, \\(d_c(f)\\).\nProof: Consider first the Fourier series representation of \\(\\delta(t)\\) in the interval \\([-1/2,1/2]\\). All its Fourier coefficients are equal to 1, hence, in that interval, \\(\\delta(t) = \\sum_n e^{2\\pi int}\\). Because all these exponentials are at least periodic with period 1, we see that \\(d_c(t) = \\sum_n \\delta(t-n) = \\sum_n e^{2\\pi int}\\). Hence, its Fourier trnasform is \\(FT[d_c(t)] = \\int_t e^{-2\\pi ift} \\sum_n \\delta(t-n) dt = \\sum_n e^{-2\\pi ifn} = d_c(f)\\).\nThe neatest part is that it can be used to model sampling. If we have a continuous signal \\(x(t)\\), and we sample it with spacing \\(\\Delta t\\), what we’re really doing is multiplying it by \\(d_c(t/\\Delta t)\\)\nIf we have a signal \\(x(t)\\) contained within a width \\(T\\) and we take the convolution \\(x(t)*d_c(t/T)\\), we are copy-pasting \\(x(t)\\) at every spike in the train, and essentially making a periodic version of the signal with period \\(T\\).",
    "crumbs": [
      "Home",
      "Physics",
      "Part 3, Unifying the continuous and discrete worlds: the dirac comb"
    ]
  },
  {
    "objectID": "physics/dirac-comb.html#deriving-the-dft",
    "href": "physics/dirac-comb.html#deriving-the-dft",
    "title": "Part 3, Unifying the continuous and discrete worlds: the dirac comb",
    "section": "Deriving the DFT",
    "text": "Deriving the DFT\nSuppose we have a sequence \\(x_n\\) which is periodic with period \\(N\\), and spaced \\(\\Delta t\\) apart, hence in the frequency domain it’s spaced \\(\\Delta f = \\frac{1}{N\\Delta t}\\) apart. The time domain representation of this sequence is \\(x(t) = \\sum_n x_n \\delta(\\frac{t}{\\Delta t} - n) = \\sum_n x_n \\Delta t \\delta(t-n\\Delta t)\\).\nUsing linearity, its Fourier transform is \\[\\begin{align}\nX(f) &= \\sum_n x_n\\Delta t e^{-2\\pi in\\Delta t f} \\\\\n&= \\sum_m \\left( \\sum_{n=0}^{N-1} x_n\\Delta t e^{-2\\pi in\\Delta t f} \\right) e^{-2\\pi imN\\Delta t f} \\\\\n&= \\left( \\sum_{n=0}^{N-1} x_n \\frac{1}{N\\Delta f} e^{-2\\pi i \\frac{n}{N}\\frac{f}{\\Delta f}} \\right) \\left( \\sum_m e^{-2\\pi im\\frac{f}{\\Delta f}} \\right) \\\\\n&= \\left( \\sum_{n=0}^{N-1} x_n \\frac{1}{N} e^{-2\\pi i \\frac{n}{N}\\frac{f}{\\Delta f}} \\right) \\frac{1}{\\Delta f} d_c(\\frac{f}{\\Delta f}) \\\\\n&= \\sum_k X_k \\delta(f-k\\Delta f)\n\\end{align}\\]",
    "crumbs": [
      "Home",
      "Physics",
      "Part 3, Unifying the continuous and discrete worlds: the dirac comb"
    ]
  },
  {
    "objectID": "physics/dirac-comb.html#applications",
    "href": "physics/dirac-comb.html#applications",
    "title": "Part 3, Unifying the continuous and discrete worlds: the dirac comb",
    "section": "Applications",
    "text": "Applications\n\nSampling\nArmed with these facts, proving the Nyquist-Shannon sampling theorem becomes trivial. Suppose you start with a signal \\(x(t)\\). Let’s suppose its Fourier transform \\(X(f)\\) is band-limited, that is, contained within an interval of width \\(F\\). Sampling with rate \\(F\\) in the time domain is equivalent to periodizing with period \\(F\\) in the frequency domain. This operation is easily reversed by multiplying by the window function of width \\(F\\) in the frequency domain, which corresponds to convolution by sinc in the time-domain, perfectly reconstructing the original continuous signal.\n\n\n\nDigital-to-analog converters (DAC)\n[TODO: figure sinc overlay of freq] Suppose we have a digital signal \\(x(t) = \\sum_{n=-\\infty}^\\infty x[n]\\delta(t/\\Delta t - n)\\). When we’re ready to output, the DAC uses zero-hold interpolation to reconstruct an analog signal. This is equivalent to convolving the digital signal by the window function \\(w_{0,\\Delta t}(t)\\). Of course, this convolution turns into multiplication by \\(\\mathrm{sinc}(\\Delta t f) e^{-\\pi i \\Delta t f}\\) in the frequency domain. Hence even though it is theoretically possible to perfectly reconstruct a band-limited continuous time signal, in practice the DAC will introduce a distortion. To deal with this distortion, it’s important both to choose the operating frequencies of DACs carefully and to place appropriate equalizing filters.\n\n\nShifting fractional frequency in discrete frequency domain.\nSuppose we have a regularly spaced grid of frequency samples, \\(X_k\\), spaced \\(\\Delta f\\) apart, that we have stored in a computer. Shifting by an integer number of spaces is easy. But what if we want to shift by a fraction of a sample, say, \\(f_s\\)? The operations we must do is as follows: 1. Convert to time-domain. This results in a signal with period \\(1/\\Delta f\\). 2. Multiply by complex exponential \\(e^{2\\pi i f_s t}\\) 3. Multiply by the window function \\(w_{-\\frac{1}{2\\Delta f},\\frac{1}{2\\Delta f}}(t)\\) 4. Make it periodic again by convolving by \\(\\sum_n \\delta(t - \\frac{n}{\\Delta f}) = \\Delta f d_c(\\Delta f t)\\) 5. Convert back to frequency domain\nIf we were to do all this in the frequency domain, it would look like: 1. Convolve by \\(\\delta(f-f_s)\\) 2. Convolve by \\(\\frac{1}{\\Delta f}\\mathrm{sinc}(\\frac{f}{\\Delta f})\\) 3. Multiply by \\(d_c(\\frac{f}{\\Delta f})\\).\nBecause convolution is associative, we can combine steps 1 and 2 so that it’s a single step, that is, convolve by \\(\\frac{1}{\\Delta f}\\mathrm{sinc}(\\frac{f-f_s}{\\Delta f})\\). Step 3 essentially samples on the same grid, and then to get the final coefficients, we need to multiply by \\(\\Delta f\\).\nIf our original sequence \\(X_k\\) was aperiodic, the shifted frequencies we eventually get is \\[\n\\begin{align}\nY_k &= \\left( \\sum_l X_l\\delta(f-l\\Delta f) \\right) * \\mathrm{sinc}(\\frac{f-f_s}{\\Delta f}) \\\\\n&= \\sum_l X_l \\mathrm{sinc}(\\frac{f-f_s-(l\\Delta f)}{\\Delta f})\\\\\n&= \\sum_l X_l \\mathrm{sinc}(k-l-\\frac{f_s}{\\Delta f})\n\\end{align}\n\\]\n\n\nTODO: Concatenating OFDM symbols, and windowing\nIn communication systems, it is common to divide the infinite line of time into so called “symbols”. For example, in 4G LTE, a symbol is about 70us. Due to the nature of the Fourier series, each of these symbols can be represented by countably many “subcarriers” regularly spaced in a frequency lattice. When these symbols are concatenated together, they are often discontinuous at the boundaries, which results in out-of-band emissions. The reason is intuitively clear. In the time domain, doing such a thing is essentially taking your periodic signal, which represents one symbol, and multiplying it by the window function. In the frequency domain, this is convoluting by a sinc function. The smaller your symbol is, the more broad the sinc function, and the more out-of-band emissions you’ll produce. However, if you smooth the window out at its boundaries, the function you convolute by in the frequency domain is more localized.",
    "crumbs": [
      "Home",
      "Physics",
      "Part 3, Unifying the continuous and discrete worlds: the dirac comb"
    ]
  },
  {
    "objectID": "physics/dirac-comb.html#todo-power",
    "href": "physics/dirac-comb.html#todo-power",
    "title": "Part 3, Unifying the continuous and discrete worlds: the dirac comb",
    "section": "TODO: Power",
    "text": "TODO: Power\nThe energy of a signal in some interval \\([a,b]\\) is defined as \\(\\int_{a}^{b} x^* (t) x(t) dt\\). The total energy, \\(\\int_{t=-\\infty}^{\\infty} x^* (t) x(t) = \\braket{x|x} = |x|^2\\), is a property of the signal itself, and therefore stays the same if we change basis into frequency domain. Hence, \\(\\int_{t=-\\infty}^{\\infty} x^* (t) x(t) = \\int_{f=-\\infty}^{\\infty} X^* (f) X(f)\\). This is called Parseval’s theorem.\nUnfortunately, energy doesn’t play so well with sampled signals, because squaring a dirac delta yields something which integrates to infinity. One trick we can play, though, is if we have a signal \\(x(t)\\), and we sample it to get \\(x(t)d_c(t/\\Delta t)\\), then an approximation of the original energy is \\(\\braket{x|x} \\approx \\braket{x(t)d_c(t/\\Delta t) | x(t)}\\).\nThe instantaneous power at time \\(t\\) is defined as the rate of energy delivered, that is, \\(x^* (t) x(t)\\). With this definition, any pure tone \\(e^{2\\pi ift}\\) has power 1.\nSuppose we wanted to plot some sort of power spectrum on a spectrum analyzer. Our instrument is finite, so let’s say it captures the signal in the interval \\([0,T]\\), and then extends the signal to infinity by periodizing. Then it gets a frequency domain plot with spacing \\(\\Delta f = 1/T\\). The total power is simply \\(\\sum_n |X(n)|^2\\). If we doubled the interval, the frequency spectrum wouldn’t change: \\(\\Delta f\\) would decrease by two, but we would now have \\(X(nf_0)=0\\) for all odd \\(n\\). Hence it makes sense to speak of the units as dBm/Hz.",
    "crumbs": [
      "Home",
      "Physics",
      "Part 3, Unifying the continuous and discrete worlds: the dirac comb"
    ]
  },
  {
    "objectID": "physics/spectral-theorem.html",
    "href": "physics/spectral-theorem.html",
    "title": "The Spectral Theorem",
    "section": "",
    "text": "The spectral theorem: Every Hermitian operator on a complex vector space has an orthonormal eigenbasis whose eigenvalues are real.\n(i.e., any Hermitian operator \\(H\\) can be written \\(H = UDU^\\dagger\\) where \\(U\\) is unitary and \\(D\\) is diagonal with real entries.)",
    "crumbs": [
      "Home",
      "Physics",
      "The Spectral Theorem"
    ]
  },
  {
    "objectID": "physics/spectral-theorem.html#theory",
    "href": "physics/spectral-theorem.html#theory",
    "title": "The Spectral Theorem",
    "section": "Theory",
    "text": "Theory\n\nBackground\nIn linear algebra, an operator on a (real or complex) vector space \\(V\\) is a linear transformation \\(T: V{\\rightarrow}V\\) mapping \\(V\\) into itself. For \\(T\\) to be called a linear transformation as opposed to just a function, it must obey the two rules of linearity: for any vectors \\(v_1, v_2 \\in V\\), and any scalar \\(c\\),\n\n\\(T(v_1+v_2) = T(v_1) + T(v_2)\\)\n\\(T(cv_1) = cT(v_1)\\)\n\nThe adjoint of an operator \\(T\\) is its conjugate-transpose.\nFor example, if \\[T =\n\\begin{pmatrix}\n1 & 2i & 3 \\\\\n2i & i & 5-4i \\\\\n-1+2i & 0 & 4-3i\n\\end{pmatrix}\n\\] its adjoint is \\[\nT^\\dagger =\n\\begin{pmatrix}\n1 & -2i & -1-2i \\\\\n-2i & -i & 0 \\\\\n3 & 5+4i & 4+3i\n\\end{pmatrix}\n\\]\nAn operator \\(H: V{\\rightarrow}V\\) is called Hermitian if it is equal to its adjoint (also known as its conjugate-transpose), written \\(H = H^\\dagger\\).\nThe example matrix \\(T\\) from earlier is not Hermitian.\nHowever, the following matrix \\(H\\) is Hermitian: \\[H =\n\\begin{pmatrix}\n1 & -2i & -1-2i \\\\\n2i & 0 & 0 \\\\\n-1+2i & 0 & -3\n\\end{pmatrix}\n\\]\nNotice in particular that if a matrix is Hermitian, its diagonal can only consist of real entries.\nFor real vector spaces, the Hermitian operators are just the symmetric operators, that is, operators which are equal to their transpose, because the conjugate of a real number is itself.\n\n\nNotation\nWhen I write \\(\\braket{u,v}\\), this means “the inner product of \\(u\\) and \\(v\\)”, and it can also be written as \\(u^\\dagger v\\). I am borrowing this from bra-ket notation, which is just a shorthand notation used a lot in quantum mechanics. To be honest I think outside of quantum mechanics, Dirac notation isn’t necessarily better in any aspects, it’s just more of a convenience.\nAlso many times in this article I focus on the vector space \\({\\bf C}^n\\), so you might be wondering whether all the results in here apply for general vector spaces as well. Actually it turns out that by specifying a basis, every finite dimensional vector space is isomorphic to \\({\\bf C}^n\\), where \\(n\\) is the dimension of the vector space. When two spaces are isomorphic it just means we can start with the vectors of one space, relabel them in some specific way to vectors of the other space, and no one can tell the difference. So actually, anything we prove about \\({\\bf C}^n\\) applies to all finite-dimensional vector spaces. In \\({\\bf C}^n\\), linear transformations are represented as matrices, so I’ll often abuse notation and say that a linear operator \\(T\\in L(V)\\) is the same as its corresponding matrix in \\({\\bf C}^n\\).\n\n\nProof for complex vector spaces\nLemma: Every linear operator on a complex vector space must have at least one eigenvalue.\nProof of lemma:\nLet \\(V\\) be an \\(n\\) dimensional complex vector space, and \\(T\\) be a linear operator. The space of linear operators of \\(V\\), written \\(L(V,V)\\) is isomorphic to the space of \\(n\\times n\\) matrices. In particular, \\(L(V,V)\\) has dimension \\(n^2\\). This means that the set\n\\[I, T, T^2, T^3,\\cdots, T^{n^2}\\]\nis linearly dependent because it has \\((n^2+1)\\) elements, which means there is a set of coefficients, not all zero, such that\n\\[c_0 + c_1 T + c_2 T^2 + \\cdots + c_{m} T^m = 0\\]\nwhere \\(c_m\\neq 0\\) and \\(m\\leq n^2\\).\nBy the fundamental theorem of algebra, this polynomial must have \\(m\\) complex roots \\(x_1,\\cdots, x_m\\), so that we can factor this into the form\n\\[(T - x_1)(T - x_2)\\cdots (T-x_m) = 0\\]\nAt least one of these factors must have a non-trivial null-space (otherwise, they would all be invertible, and their product would be invertible and hence non-zero). Let’s say \\((T-x_i)\\) has a non-trivial null-space. Then \\(x_i\\) is the eigenvalue of \\(T\\) we are looking for.\nProof of rest of theorem:\nLet \\(V\\) be an \\(n\\)-dimensional complex inner product space, and \\(H\\) be a Hermitian operator. It must have some eigenvalue, let’s call it \\(\\lambda\\). Consider the subspace \\(S_\\lambda\\) consisting of all vectors which have \\(\\lambda\\) as their eigenvalue. Because of its definition, \\(S_\\lambda\\) is invariant under \\(H\\), since for any \\(v\\in S_\\lambda\\), \\(Hv = \\lambda v \\in S_\\lambda\\) also has eigenvalue \\(\\lambda\\), so \\(H S_\\lambda \\subset S_\\lambda\\).\nNow consider the orthogonal complement \\(S_\\lambda^\\perp\\), which consists of the vectors in \\(V\\) which are orthogonal to every vector in \\(S_\\lambda\\). These two subspaces together make up the vector space, so that we can write \\(V = S_\\lambda \\oplus S_\\lambda^\\perp\\). This symbolic expression means that any vector in \\(V\\) can be decomposed uniquely into the sum of a vector from \\(S_\\lambda\\) and a vector from \\(S_\\lambda^\\perp\\).\nHere is the crucial part of the proof which uses the fact that \\(H\\) is Hermitian. If \\(u\\in S_\\lambda^\\perp\\), that means \\(u\\) is orthogonal to every vector \\(v\\in S_\\lambda\\), so that\n\\[\\braket{v|Hu} = \\braket{Hv|u} = \\lambda^*\\braket{u|v} = 0\\]\nso \\(Hu\\in S_\\lambda^\\perp\\). This means that \\(H S_\\lambda^\\perp \\subset S_\\lambda^\\perp\\), so that \\(S_\\lambda^\\perp\\) is invariant.\nHence we can separate our operator on its restriction to the two subspaces: \\(H = (H| _ {S _ \\lambda}) \\oplus (H| _ {S _ \\lambda ^ \\perp})\\)\nThe resulting matrix representation of \\(H\\) now looks something like this:\n\n(The exact shape of the matrix will depend on the dimension of eigenspace \\(S_\\lambda\\); this dimension is called the multiplicity of \\(\\lambda\\)).\nWe can now repeat this process for the smaller matrix \\(H| _ {S _ \\lambda^\\perp}\\) (i.e. \\(H\\) restricted to the subspace \\(S_\\lambda^\\perp\\)). When we can repeat no longer, we end up with a decomposition of \\(H\\) into its restriction on its eigenspaces:\n\nIf we look only at what \\(H\\) does on each individual eigenspace \\(S _ {\\lambda _ i}\\), \\(H\\) just scales each vector in the eigenspace by its eigenvalue \\(\\lambda\\), that is, \\(H| _ { S _ {\\lambda _ i} } = \\lambda_i I\\). So if we just choose any orthonormal basis for each subspace (for example, using the Gramd-Schmidt procedure), we can re-express \\(H\\) in this basis as the matrix\n\nIn this basis we have constructed made out of the eigenvectors, \\(H\\) is simply a diagonal matrix with its eigenvalues along the diagonal.\nEven though we’ve changed basis, the fact that \\(H\\) is Hermitian has not changed (the fancy way to say this is that Hermitianess is a property which is invariant under an orthogonal change of basis). So the diagonal matrix pictured above is Hermitian. This means the entries along the diagonal must be equal to their conjugate, and hence real. This means that the eigenvalues of \\(H\\) are real, so we are finished with the proof.\n\n\n\n\n\n\nNoteModifications for real vector spaces\n\n\n\n\n\nFor real vector spaces, Hermitian operators are simply symmetric operators, that is, operators who are equal to their transpose: \\(H = H^T\\).\nUnlike in the complex case, not every operator on a real vector space has eigenvalues. For example, consider the following matrix, which rotates vectors by 90 degrees counter-clockwise.\n\\[\nR = \\begin{pmatrix}\n0 & -1 \\\\\n1 & 0\n\\end{pmatrix}\n\\]\nViewed as an operator on \\({\\bf R}^2\\), \\(R\\) rotates every vector in \\({\\bf R}^2\\), so it leaves no vector pointing in the same direction, hence has no eigenvalues.\nAlso if we take the real part of two complex vectors, they are not necessarily orthonormal. For example,\n\\[\nA = \\begin{pmatrix}\n1 & 2-i \\\\\n1-2i & i\n\\end{pmatrix}\n\\]\nis orthonormal in \\({\\bf C}^2\\), but\n\\[\n{\\operatorname{Re}}(A) = \\begin{pmatrix}\n1 & 2 \\\\\n1 & 0\n\\end{pmatrix}\n\\]\nis not orthonormal in \\({\\bf R}^2\\)\nFor the special case of symmetric matrices, however, we can piggy-back on our lemma for the complex case. If \\(H\\) is a symmetric operator in \\(L({\\bf R}^n)\\), then if we view it as an operator \\(L({\\bf C}^n)\\), it is Hermitian, so we can find a real eigenvalue \\(\\lambda\\) and complex eigenvector \\(v = v_R + i v_I\\).\n\\[\n\\begin{align*}\nHv &= \\lambda v \\\\\n{\\operatorname{Re}}(Hv) &= {\\operatorname{Re}}(\\lambda v) \\\\\nH {\\operatorname{Re}}(v) &= \\lambda {\\operatorname{Re}}(v)\n\\end{align*}\n\\]\nNote how this manipulation relies on the fact that both \\(H\\) and \\(\\lambda\\) are real.\nAfter we establish the existence of a single eigenvalue for symmetric operators on real vector spaces, the rest of the proof is identical to the complex case.",
    "crumbs": [
      "Home",
      "Physics",
      "The Spectral Theorem"
    ]
  },
  {
    "objectID": "physics/spectral-theorem.html#finite-dimensional-applications",
    "href": "physics/spectral-theorem.html#finite-dimensional-applications",
    "title": "The Spectral Theorem",
    "section": "Finite dimensional applications",
    "text": "Finite dimensional applications\n\nRotational inertia tensor\nA rigid body is a system of masses where the distances of each mass to one another are fixed. For such a rigid body which is fixed with respect to the origin, we can identify an axis of rotation \\(\\bf{\\omega}\\) called the angular velocity such that the velocity of any particle \\(\\bf{v_i}\\) is given by \\(\\bf{v_i} = \\bf{\\omega} \\times \\bf{r_i}\\).\nThe angular momentum of a single particle is defined as \\(\\bf{L} = \\bf{r}\\times \\bf{p}\\) where \\(\\bf{r}\\) is the position vector of the particle and \\(p\\) is its momentum.\nThe total angular momentum of the body is given by\n\\[\n\\begin{align*}\n\\bf{L} &= \\sum_i m_i \\bf{r_i}\\times \\bf{v_i} \\\\\n&= \\sum_i m_i \\bf{r_i}\\times (\\bf{\\omega} \\times \\bf{r_i})\n\\end{align*}\n\\]\nFrom the equation we can see that \\(\\bf{L}\\) depends linearly on \\(\\omega\\). If we expand out the individual components, we get the following linear relation:\n\\[\n\\begin{pmatrix}\nL_x \\\\\nL_y \\\\\nL_z\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\sum_i m_i (y_i^2 + z_i^2) & -\\sum_i m_i x_i y_i & -\\sum_i m_i x_i z_i \\\\\n-\\sum_i m_i x_i y_i & \\sum_i m_i (z_i^2 + x_i^2) & -\\sum_i m_i y_i z_i \\\\\n-\\sum_i m_i x_i z_i & -\\sum_i m_i y_i z_i & \\sum_i m_i (x_i^2 + y_i^2)\n\\end{pmatrix}\n\\begin{pmatrix}\n\\omega_x \\\\\n\\omega_y \\\\\n\\omega_z\n\\end{pmatrix}\n\\]\nHence we can write \\(\\bf{L} = I \\bf{\\omega}\\), where \\(I\\) is called the tensor of inertia (in this context, tensor really means second-order tensor, which is just a linear transformation, and can be represented as a matrix). Note that \\(I\\) is a symmetric tensor. By the spectral theorem, we can find an orthonormal basis \\(\\hat{\\bf{x}}', \\hat{\\bf{y}}', \\hat{\\bf{z}}'\\) such that the relation looks like this:\n\\[\n\\begin{pmatrix}\nL_x' \\\\\nL_y'\\\\\nL_z'\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\lambda_1 & 0 & 0 \\\\\n0 & \\lambda_2 & 0 \\\\\n0 & 0 & \\lambda_3\n\\end{pmatrix}\n\\begin{pmatrix}\n\\omega_x' \\\\\n\\omega_y' \\\\\n\\omega_z'\n\\end{pmatrix}\n\\]\n\n\nNormal modes\n\noriginal image\ncoupled pendulum demo\nThe x-component of the gravitational force acting on the first mass is \\(-m_1 g x_1/L\\). The spring exerts a force of \\(k(x_2-x_1)\\). Similar equations hold for the second mass. Hence we can use \\(F_x = m a_x\\) for the two masses to write two equations describing the dynamics:\n\\[\n\\begin{align*}\n(-m_1 g x_1/L) + k(x_2-x_1) &= m_1 \\tfrac{d^2}{dt^2} x_1 \\\\\n(-m_2 g x_2/L) - k(x_2-x_1) &= m_2 \\tfrac{d^2}{dt^2} x_2\n\\end{align*}\n\\]\nWe can write this in matrix form:\n\\[\n\\tfrac{d^2}{dt^2}\n\\begin{pmatrix}\nx_1 \\\\\nx_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n-g/L - k/m_1 & k/m_1 \\\\\nk/m_2 & -g/L - k/m_2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\\nx_2\n\\end{pmatrix}\n\\]\n\n\nSingular value decomposition (SVD)\n\nhttps://en.wikipedia.org/wiki/File:Singular-Value-Decomposition.svg\n\n\nPrinciple component analysis (PCA)\n https://en.wikipedia.org/wiki/File:GaussianScatterPCA.svg\nSuppose we have a random vector \\(X\\), and we make a bunch of observations \\(x_1, \\cdots, x_P\\). We can organize this into a matrix\n\\[\nM = \\begin{pmatrix}\n\\vdots & & \\vdots \\\\\nx_1 & \\dots & x_P \\\\\n\\vdots & & \\vdots\n\\end{pmatrix}\n\\]\nThe sample covariance matrix is defined \\(K_{XX} = \\frac{1}{P-1} MM^T\\). This is symmetric! By the spectral theorem, we can find an orthornormal basis in which \\(X\\) looks uncorrelated, i.e., find an orthonormal matrix \\(U\\) such that \\(Y = U^{-1} X\\) is uncorrelated: \\(K_{YY}\\) is diagonal.",
    "crumbs": [
      "Home",
      "Physics",
      "The Spectral Theorem"
    ]
  },
  {
    "objectID": "physics/spectral-theorem.html#infinite-dimensional-applications",
    "href": "physics/spectral-theorem.html#infinite-dimensional-applications",
    "title": "The Spectral Theorem",
    "section": "Infinite dimensional applications",
    "text": "Infinite dimensional applications\n\nFourier transform\nIt turns out the operator \\(\\tfrac{1}{2\\pi i}\\frac{d}{dt}\\) is Hermitian in \\(L^2({\\bf R})\\). We can verify this by expanding out the integrals and using integration by parts.\n\\[\n\\begin{align*}\n\\braket{x(t)|\\tfrac{1}{2\\pi i}\\tfrac{d}{dt}|y(t)} &= \\int _ {-\\infty}^\\infty x ^ *(t) \\tfrac{1}{2\\pi i}\\tfrac{d}{dt} y(t) dt \\\\\n&= [\\tfrac{1}{2\\pi i} x ^ *(t) y(t)] _ {-\\infty}^\\infty - \\int_{-\\infty}^\\infty y(t) \\tfrac{1}{2\\pi i}\\tfrac{d}{dt}x ^ *(t) dt \\\\\n&= \\braket{y(t)|\\tfrac{1}{2\\pi i}\\tfrac{d}{dt}|x(t)} ^ *\n\\end{align*}\n\\]\n(The second step used the fact that \\(x,y\\) were square-integrable functions, so they have to vanish at infinity, so \\([\\tfrac{1}{2\\pi i} x(t)^* y(t)]_{-\\infty}^\\infty = 0\\))\nBecause it is Hermitian, the spectral theorem says there is a complete set of orthonormal eigenvectors. These eigenvectors are found by solving the equation\n\\[\n(\\tfrac{1}{2\\pi i}\\tfrac{d}{dt}) x(t) = \\lambda x(t)\n\\]\nwhich has only one solution: \\[\nx(t) = e^{2\\pi i\\lambda t}\n\\]\nHence the eigenvectors are complex sinusoids, and the corresponding eigenvalue is the frequency of the complex sinusoid.\n\n\nVibrating Membranes\n\nCan one hear the shape of a drum?\n— Mark Kac, 1966, American Mathematical Monthly\n\nSuppose we have a simple region \\(\\Omega\\) in the plane, and we stretch a membrane on it. The wave equation for vibrating membranes is\n\\[\n(\\partial_x^2 + \\partial_y^2) u = \\frac{1}{c^2}\\partial_t^2 u\n\\]\nwhere \\(u(t, x,y)\\) is height and \\(c\\) is the speed of waves on the membrane. We require \\(u=0\\) on the boundary of the drum. The Hilbert space under study is the set of square-integrable functions \\(u: {\\bf R}\\times \\Omega {\\rightarrow}{\\bf R}\\) which are 0 on the boundary of \\(\\Omega\\) at all times. We solve this by finding the eigenvalues of \\((\\partial_x^2 + \\partial_y^2)\\).\n\n\n\nSpherical Harmonics\nThe Laplacian in spherical coordinates is:\n\\[\n\\begin{pmatrix}\ndx \\\\\ndy \\\\\ndz\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\sin\\theta \\cos\\varphi & r \\cos\\theta \\cos\\varphi & -r \\sin\\theta \\sin\\varphi \\\\\n\\sin\\theta \\sin\\varphi & r \\cos\\theta \\sin\\varphi & r \\sin\\theta \\cos\\varphi \\\\\n\\cos\\theta & -r \\sin\\theta & 0\n\\end{pmatrix}\n\\begin{pmatrix}\ndr \\\\\nd\\theta \\\\\nd\\varphi\n\\end{pmatrix}\n\\]\n\\[\n\\begin{pmatrix}\ndx \\\\\ndy \\\\\ndz\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\sin\\theta \\cos\\varphi & \\cos\\theta \\cos\\varphi & -\\sin\\varphi \\\\\n\\sin\\theta \\sin\\varphi & \\cos\\theta \\sin\\varphi & \\cos\\varphi \\\\\n\\cos\\theta & -\\sin\\theta & 0\n\\end{pmatrix}\n\\begin{pmatrix}\ndr \\\\\nrd\\theta \\\\\nr\\sin\\theta d\\varphi\n\\end{pmatrix}\n\\]\n\\[\n\\begin{pmatrix}\ndr \\\\\nrd\\theta \\\\\nr\\sin\\theta d\\varphi\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\sin\\theta \\cos\\varphi & \\sin\\theta \\sin\\varphi & \\cos\\theta \\\\\n\\cos\\theta \\cos\\varphi & \\cos\\theta \\sin\\varphi & -\\sin\\theta \\\\\n-\\sin\\varphi & \\cos\\varphi & 0\n\\end{pmatrix}\n\\begin{pmatrix}\ndx \\\\\ndy \\\\\ndz\n\\end{pmatrix}\n\\]\n\\[\n\\begin{pmatrix}\ndr \\\\\nd\\theta \\\\\nd\\varphi\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\sin\\theta \\cos\\varphi & \\sin\\theta \\sin\\varphi & \\cos\\theta \\\\\n\\tfrac{1}{r}\\cos\\theta \\cos\\varphi & \\tfrac{1}{r}\\cos\\theta \\sin\\varphi & -\\tfrac{1}{r}\\sin\\theta \\\\\n-\\tfrac{1}{r\\sin\\theta}\\sin\\varphi & \\tfrac{1}{r\\sin\\theta}\\cos\\varphi & 0\n\\end{pmatrix}\n\\begin{pmatrix}\ndx \\\\\ndy \\\\\ndz\n\\end{pmatrix}\n\\]\nSuppose\n\\[\n\\nabla^2 f = \\frac{1}{r^2}{\\frac{\\partial }{\\partial r}}\\left ( r^2{\\frac{\\partial f}{\\partial r}} \\right ) + \\frac{1}{r^2\\sin\\theta}{\\frac{\\partial }{\\partial \\theta}} \\left ( \\sin\\theta{\\frac{\\partial f}{\\partial \\theta}} \\right ) + \\frac{1}{r^2\\sin^2\\theta}{\\frac{\\partial^2 f}{\\partial^2 \\varphi}}\n\\]\nNow suppose \\(f(r, \\theta, \\varphi) = R(r)Y(\\theta, \\phi)\\). Plugging it in, we get\n\\[\nf \\mapsto \\frac{1}{\\sin\\theta}{\\frac{\\partial }{\\partial \\theta}} \\left ( \\sin\\theta{\\frac{\\partial f}{\\partial \\theta}} \\right ) + \\frac{1}{\\sin^2\\theta}{\\frac{\\partial^2 f}{\\partial^2 \\varphi}}\n\\]\nwe can separate coordinates to get a tensor-product separation:\n\nimage source: https://en.wikipedia.org/wiki/File:Rotating_spherical_harmonics.gif\n image source: https://en.wikipedia.org/wiki/File:Sphericalfunctions.svg\n\n\nMaxwell’s Equations\n\\[\n\\begin{align*}\n\\nabla \\cdot E &= \\rho / \\epsilon_0 \\\\\n\\nabla \\cdot B &= 0 \\\\\n\\nabla \\times E &= -{\\frac{\\partial B}{\\partial t}} \\\\\n\\nabla \\times B &= \\mu_0 J + \\epsilon_0 \\mu_0 {\\frac{\\partial E}{\\partial t}}\n\\end{align*}\n\\]\nExpanded out into components, this becomes\n\\[\n\\begin{align*}\n\\partial_y E_z - \\partial_z E_y &= -\\partial_t B_x \\\\\n\\partial_z E_x - \\partial_x E_z &= -\\partial_t B_y \\\\\n\\partial_x E_y - \\partial_y E_x &= -\\partial_t B_z \\\\\n\\partial_y B_z - \\partial_z B_y &= \\mu_0 J_x + \\epsilon_0 \\mu_0 \\partial_t E_x \\\\\n\\partial_z B_x - \\partial_x B_z &= \\mu_0 J_y + \\epsilon_0 \\mu_0 \\partial_t E_y \\\\\n\\partial_x B_y - \\partial_y B_x &= \\mu_0 J_z + \\epsilon_0 \\mu_0 \\partial_t E_z\n\\end{align*}\n\\]\n\\[\n\\bigoplus_6 \\bigotimes_{3+1} L^2({\\bf R}) = \\bigoplus_6 L^2({\\bf R}^{3+1})\n\\]\n\\[\n\\braket{u_1\\oplus v_1 | u_2\\oplus v_2} = \\braket{u_1|u_2} + \\braket{v_1|v_2}\n\\] \\[\n\\braket{u_1\\otimes v_1 | u_2\\otimes v_2} = \\braket{u_1|u_2} \\braket{v_1|v_2}\n\\]\n\\[\n\\begin{pmatrix}\n0 & -\\partial_z & +\\partial_y & +\\frac{1}{c}\\partial_t & 0 & 0 \\\\\n+\\partial_z & 0 & -\\partial_x & 0 & +\\frac{1}{c}\\partial_t & 0 \\\\\n-\\partial_y & +\\partial_x & 0 & 0 & 0 & +\\frac{1}{c}\\partial_t \\\\\n-\\frac{1}{c}\\partial_t & 0 & 0 & 0 & -\\partial_z & +\\partial_y \\\\\n0 & -\\frac{1}{c}\\partial_t & 0 & +\\partial_z & 0 & -\\partial_x \\\\\n0 & 0 & -\\frac{1}{c}\\partial_t & -\\partial_y & +\\partial_x & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nE_x \\\\\nE_y \\\\\nE_z \\\\\ncB_x \\\\\ncB_y \\\\\ncB_z\n\\end{pmatrix} = \\begin{pmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\nc\\mu_0 J_x \\\\\nc\\mu_0 J_y \\\\\nc\\mu_0 J_z\n\\end{pmatrix}\n\\]\nWe already concluded that \\(\\tfrac{1}{2\\pi i} \\partial_x\\) is Hermitian, so we know that \\((i\\partial_x)\\) is also Hermitian since it’s just the first operator scaled by the real number \\(-2\\pi\\). Hence we can work out the adjoint of the derivative operator:\n\\[\n\\begin{align*}\n(i\\partial_x)^\\dagger &= i\\partial_x \\\\\ni (i\\partial_x)^\\dagger &= i^2\\partial_x \\\\\n(-i i \\partial_x)^\\dagger &= -\\partial_x \\\\\n(\\partial_x)^\\dagger &= -\\partial_x\n\\end{align*}\n\\]\nSuppose we have two vector spaces \\(U, V\\). An arbitrary linear transformation \\(A\\) on the vector space \\(U\\oplus V\\) can be written as\n\\[\n\\begin{pmatrix}\nA_{uu} & A_{uv} \\\\\nA_{vu} & A_{vv}\n\\end{pmatrix}\n\\begin{pmatrix}\nu \\\\\nv\n\\end{pmatrix} =\n(A_{uu} u + A_{uv} v) \\oplus (A_{vu} u + A_{vv} v)\n\\]\nUnder what circumstances is \\(A\\) Hermitian?\n\\[\n\\begin{pmatrix}\nu_1^\\dagger & v_1^\\dagger\n\\end{pmatrix}\n\\begin{pmatrix}\nA_{uu} & A_{uv} \\\\\nA_{vu} & A_{vv}\n\\end{pmatrix}\n\\begin{pmatrix}\nu_2 \\\\\nv_2\n\\end{pmatrix} = \\left [\n\\begin{pmatrix}\nu_2^\\dagger & v_2^\\dagger\n\\end{pmatrix}\n\\begin{pmatrix}\nA_{uu} & A_{uv} \\\\\nA_{vu} & A_{vv}\n\\end{pmatrix}\n\\begin{pmatrix}\nu_1 \\\\\nv_1\n\\end{pmatrix} \\right ]^*\n\\]\nThis requires \\(A_{uu}, A_{vv}\\) to be Hermitian, and \\(A_{uv} = A_{vu}^\\dagger\\)",
    "crumbs": [
      "Home",
      "Physics",
      "The Spectral Theorem"
    ]
  },
  {
    "objectID": "physics/spectral-theorem.html#further-reading",
    "href": "physics/spectral-theorem.html#further-reading",
    "title": "The Spectral Theorem",
    "section": "Further Reading",
    "text": "Further Reading\n\nIntroduction to Quantum Mechanics, David Griffiths. Even though it only discusses quantum mechanics, the math it develops can be applied to pretty much anything that uses waves, since all wave equations are essentially the same. The spectral theorem is so essential to quantum mechanics that it is discussed in depth there. This book is accessible to anyone with knowledge of basic calculus and linear algebra. It’s a legendary physics textbook, known for its clarity and its honest discussions of deep matters.\nMathematics of Classical and Quantum Physics\nReal and Complex Analysis, Walter Rudin. Very rigorous, do not read unless you have taken an intro to real analysis course. I have personally only glanced through it, my impression is that infinite dimensional Hilbert spaces are much more nuanced than finite dimensional ones.\nLinear Algebra Done Right, Sheldon Axler. The proof of the spectral theorem in this article was taken from this book.\nAdvanced Linear Algebra, Roman. The first few chapters give a superb introduction to the abstract side of vector spaces.\nLinear Algebra and Its Applications 5th edition, David Lay, Steven Lay, Judi McDonald. The singular value decomposition is explained very well in chapter 7.",
    "crumbs": [
      "Home",
      "Physics",
      "The Spectral Theorem"
    ]
  }
]